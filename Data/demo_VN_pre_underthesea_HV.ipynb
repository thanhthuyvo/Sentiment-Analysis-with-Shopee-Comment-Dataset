{"cells":[{"cell_type":"code","source":["# from google.colab import drive\n","# drive.mount(\"/content/gdrive\", force_remount=True)\n","\n","# %cd '/content/gdrive/My Drive/LDS0/Topic_2_3_2_1/demo/'"],"metadata":{"id":"cGy1fE8h2g2m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install underthesea\n","!pip install demoji\n","!pip install pyvi"],"metadata":{"id":"63Bhl4Ey2euX"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NoveTrE9tGAA"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from underthesea import word_tokenize, pos_tag, sent_tokenize\n","import regex\n","import demoji\n","from pyvi import ViPosTagger, ViTokenizer\n","import string"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"98pIaiUdtGAE"},"outputs":[],"source":["##LOAD EMOJICON\n","file = open('files/emojicon.txt', 'r', encoding=\"utf8\")\n","emoji_lst = file.read().split('\\n')\n","emoji_dict = {}\n","for line in emoji_lst:\n","    key, value = line.split('\\t')\n","    emoji_dict[key] = str(value)\n","file.close()\n","#################\n","#LOAD TEENCODE\n","file = open('files/teencode.txt', 'r', encoding=\"utf8\")\n","teen_lst = file.read().split('\\n')\n","teen_dict = {}\n","for line in teen_lst:\n","    key, value = line.split('\\t')\n","    teen_dict[key] = str(value)\n","file.close()\n","###############\n","#LOAD TRANSLATE ENGLISH -> VNMESE\n","file = open('files/english-vnmese.txt', 'r', encoding=\"utf8\")\n","english_lst = file.read().split('\\n')\n","english_dict = {}\n","for line in english_lst:\n","    key, value = line.split('\\t')\n","    english_dict[key] = str(value)\n","file.close()\n","################\n","#LOAD wrong words\n","file = open('files/wrong-word.txt', 'r', encoding=\"utf8\")\n","wrong_lst = file.read().split('\\n')\n","file.close()\n","#################\n","#LOAD STOPWORDS\n","file = open('files/vietnamese-stopwords.txt', 'r', encoding=\"utf8\")\n","stopwords_lst = file.read().split('\\n')\n","file.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Od1WjpEjtGAE"},"outputs":[],"source":["def process_text(text, emoji_dict, teen_dict, wrong_lst):\n","    document = text.lower()\n","    document = document.replace(\"’\",'')\n","    document = regex.sub(r'\\.+', \".\", document)\n","    new_sentence =''\n","    for sentence in sent_tokenize(document):\n","        # if not(sentence.isascii()):\n","        ###### CONVERT EMOJICON\n","        sentence = ''.join(emoji_dict[word]+' ' if word in emoji_dict else word for word in list(sentence))\n","        ###### CONVERT TEENCODE\n","        sentence = ' '.join(teen_dict[word] if word in teen_dict else word for word in sentence.split())\n","        ###### DEL Punctuation & Numbers\n","        pattern = r'(?i)\\b[a-záàảãạăắằẳẵặâấầẩẫậéèẻẽẹêếềểễệóòỏõọôốồổỗộơớờởỡợíìỉĩịúùủũụưứừửữựýỳỷỹỵđ]+\\b'\n","        sentence = ' '.join(regex.findall(pattern,sentence))\n","        ###### DEL wrong words   \n","        sentence = ' '.join('' if word in wrong_lst else word for word in sentence.split())\n","        new_sentence = new_sentence+ sentence + '. '                    \n","    document = new_sentence  \n","    #print(document)\n","    ###### DEL excess blank space\n","    document = regex.sub(r'\\s+', ' ', document).strip()\n","    return document"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"So0KldAgtGAG"},"outputs":[],"source":["# Chuẩn hóa unicode tiếng việt\n","def loaddicchar():\n","    uniChars = \"àáảãạâầấẩẫậăằắẳẵặèéẻẽẹêềếểễệđìíỉĩịòóỏõọôồốổỗộơờớởỡợùúủũụưừứửữựỳýỷỹỵÀÁẢÃẠÂẦẤẨẪẬĂẰẮẲẴẶÈÉẺẼẸÊỀẾỂỄỆĐÌÍỈĨỊÒÓỎÕỌÔỒỐỔỖỘƠỜỚỞỠỢÙÚỦŨỤƯỪỨỬỮỰỲÝỶỸỴÂĂĐÔƠƯ\"\n","    unsignChars = \"aaaaaaaaaaaaaaaaaeeeeeeeeeeediiiiiooooooooooooooooouuuuuuuuuuuyyyyyAAAAAAAAAAAAAAAAAEEEEEEEEEEEDIIIOOOOOOOOOOOOOOOOOOOUUUUUUUUUUUYYYYYAADOOU\"\n","\n","    dic = {}\n","    char1252 = 'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ'.split(\n","        '|')\n","    charutf8 = \"à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ\".split(\n","        '|')\n","    for i in range(len(char1252)):\n","        dic[char1252[i]] = charutf8[i]\n","    return dic\n"," \n","# Đưa toàn bộ dữ liệu qua hàm này để chuẩn hóa lại\n","def covert_unicode(txt):\n","    dicchar = loaddicchar()\n","    return regex.sub(\n","        r'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ',\n","        lambda x: dicchar[x.group()], txt)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"92LeWTwdtGAI"},"outputs":[],"source":["# có thể bổ sung thêm các từ: chẳng, chả...\n","def process_special_word(text):\n","    new_text = ''\n","    text_lst = text.split()\n","    i= 0\n","    if 'không' in text_lst:\n","        while i <= len(text_lst) - 1:\n","            word = text_lst[i]\n","            #print(word)\n","            #print(i)\n","            if  word == 'không':\n","                next_idx = i+1\n","                if next_idx <= len(text_lst) -1:\n","                    word = word +'_'+ text_lst[next_idx]\n","                i= next_idx + 1\n","            else:\n","                i = i+1\n","            new_text = new_text + word + ' '\n","    else:\n","        new_text = text\n","    return new_text.strip()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y4C_qefxtGAJ"},"outputs":[],"source":["def process_postag_thesea(text):\n","    new_document = ''\n","    for sentence in sent_tokenize(text):\n","        sentence = sentence.replace('.','')\n","        ###### POS tag\n","        lst_word_type = ['A','AB','V','VB','VY','R']\n","        sentence = ' '.join( word[0] if word[1].upper() in lst_word_type else '' for word in pos_tag(process_special_word(word_tokenize(sentence, format=\"text\"))))\n","        new_document = new_document + sentence + ' '\n","    ###### DEL excess blank space\n","    new_document = regex.sub(r'\\s+', ' ', new_document).strip()\n","    return new_document"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cWPpD0qqtGAK"},"outputs":[],"source":["def remove_stopword(text, stopwords):\n","    ###### REMOVE stop words\n","    document = ' '.join('' if word in stopwords else word for word in text.split())\n","    #print(document)\n","    ###### DEL excess blank space\n","    document = regex.sub(r'\\s+', ' ', document).strip()\n","    return document"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bnAc6joutGAF"},"outputs":[],"source":["example = '''1m6 50kg size M khá vừa vặn nhưng hok có cơ nên nhìn cx thường 😊. Nâu với trắng vải dày hơn đỏ, trắng với đỏ cổ cao hơn nâu một tí, trắng đẹp 👍. \n","Vai áo chưa đẹp lắm. Khá ổn so với giá tiền'''"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"nbformat":4,"nbformat_minor":0}